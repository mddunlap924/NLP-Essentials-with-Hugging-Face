{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"intro\">Text Tokenization - Three Data Splitting Approaches</h1>\n",
    "\n",
    "This notebook provides a walkthrough of chunking and splitting text with a Hugging Face tokenizer. The concepts in this notebook can be used to gain familiarity with tokenizers which are fundamental for NLP.\n",
    "\n",
    "## Table of Contents\n",
    "If viewing this notebook from GitHub please view it instead on [nbviewer.org](https://nbviewer.org/) so the hyperlinks will function. \n",
    "\n",
    "- [User Inputs](#user-inputs)\n",
    "- [Import Libraries and Modules](#import-libs)\n",
    "- [Tokenizer](#tokenizer)\n",
    "- [Approach 1: Right Side Truncation](#approach-1-right-side-truncation)\n",
    "- [Approach 2: Right and Left Side of Text](#approach-2-right-left-side-truncation)\n",
    "- [Approach 3: Chunking with Overlap](#approach-3-chunking-overlap)\n",
    "- [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"user-inputs\">User Inputs</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'gemma-2-9b-it'\n",
    "\n",
    "# Tokenizer max length and stride\n",
    "max_length = 7\n",
    "stride = 2\n",
    "\n",
    "# Example text to use for demonstration\n",
    "text = (f'This is text that will be chunked with '\n",
    "        f'overlap for simple example use cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"import-libs\">Import Libraries and Modules</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Setup HF env. variables\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tokenizer\">Tokenizer</h1>\n",
    "\n",
    "The Gemma 2 model uses a [SentencePiece tokenizer](https://arxiv.org/html/2408.00118v1) with a vocabulary size of 256K.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token in Vocabulary: 256,000\n"
     ]
    }
   ],
   "source": [
    "# Path to the model and tokenizer model card saved on disk\n",
    "model_path = Path(os.getenv('LLM_MODELS')) / model_name\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Number of tokens in the tokenizer\n",
    "print(f'Number of Token in Vocabulary: {len(tokenizer.get_vocab()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in Hugging Face tokenizers are used to indicate the start and end of a sequence, separate sentences, or represent padding and unknown tokens. These tokens help the model understand the structure and boundaries of the input text during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens\n",
      "bos_token: <bos> -> 2\n",
      "eos_token: <eos> -> 1\n",
      "unk_token: <unk> -> 3\n",
      "sep_token: None -> None\n",
      "pad_token: <pad> -> 0\n",
      "cls_token: None -> None\n",
      "mask_token: None -> None\n",
      "additional_special_tokens: ['<start_of_turn>', '<end_of_turn>'] -> None\n",
      "\n",
      "Additional Special Tokens\n",
      "106: <start_of_turn>\n",
      "107: <end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "print('Special Tokens')\n",
    "for token_name in tokenizer.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "    token = getattr(tokenizer, token_name)\n",
    "    if token_name == 'additional_special_tokens':\n",
    "        token_id = None\n",
    "    else:\n",
    "        token_id = getattr(tokenizer, f'{token_name}_id')\n",
    "    print(f'{token_name}: {token} -> {token_id}')\n",
    "\n",
    "# Additional special tokens\n",
    "print(f'\\nAdditional Special Tokens')\n",
    "for token_id in tokenizer.additional_special_tokens_ids:\n",
    "    print(f'{token_id}: {tokenizer.decode(token_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def view_tokens(tok: Type[T], input_ids: list) -> None:\n",
    "    \"\"\"Decode and Print input_ids tokens for viewing \n",
    "\n",
    "    Args:\n",
    "        tok (Type[T]): Tokenizer\n",
    "        input_ids (list): List of input_id tokens\n",
    "    \"\"\"\n",
    "    print(f'input_ids:\\n{input_ids}\\n\\n')\n",
    "    print(f'tokenizer.convert_ids_to_tokens(input_ids):\\n'\n",
    "          f'{tok.convert_ids_to_tokens(input_ids)}\\n\\n')\n",
    "    \n",
    "    print(f'tokenizer.decode(input_ids):\\n'\n",
    "          f'{tok.decode(input_ids)}\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tk): 3\n",
      "\n",
      "tk.keys(): dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "\n",
      "Number of Tokens: 16\n",
      "\n",
      "input_ids:\n",
      "[2, 1596, 603, 2793, 674, 877, 614, 788, 129632, 675, 40768, 604, 3890, 3287, 1281, 4381]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', 'This', '▁is', '▁text', '▁that', '▁will', '▁be', '▁ch', 'unked', '▁with', '▁overlap', '▁for', '▁simple', '▁example', '▁use', '▁cases']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos>This is text that will be chunked with overlap for simple example use cases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tk = tokenizer(text,\n",
    "               truncation=False,\n",
    "               return_length=True)\n",
    "\n",
    "# Number of tokens in the text\n",
    "total_tokens_text = tk['length'][0]\n",
    "\n",
    "# Total tokens in text\n",
    "print(f'len(tk): {len(tk)}\\n')\n",
    "print(f'tk.keys(): {tk.keys()}\\n')\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-1-right-side-truncation\">Approach 1: Right Side Truncation</h1>\n",
    "\n",
    "\n",
    "This apporach takes the first portion of text (from 0 to max_length) and is the most common approach found in tokenizing text.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 7\n",
      "\n",
      "input_ids:\n",
      "[2, 1596, 603, 2793, 674, 877, 614]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', 'This', '▁is', '▁text', '▁that', '▁will', '▁be']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos>This is text that will be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep the right side of the text\n",
    "tk = tokenizer(text,\n",
    "               max_length=max_length,\n",
    "               truncation=True,\n",
    "               return_length=True,\n",
    "               padding=False)\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-2-right-left-side-truncation\">Approach 2: Right and Left Side Truncation</h1>\n",
    "\n",
    "In this approach the right and left side of the text will be taken and combined together. This will require two separate tokenizations of the text, recombining it, and then tokenizing in the chat template.\n",
    "\n",
    "Some math needs to be performed to account for the max_length to take from each side. When doing this account for:\n",
    "- Number of tokens to take for each side (i.e., max_length/2),\n",
    "- The `<bos>` token will be dropped at the start of each side of text.\n",
    "\n",
    "In the below cell notice the use of `math.floor` and `math.ceil` which are used if an odd `max_length` is set. This will ensure that the returned number of tokens is exactly `max_length`. \n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user specified -> max_length: 7\n",
      "max_len_right: 4\n",
      "max_len_left: 4\n",
      "actual_right: 3\n",
      "actual_left: 3\n",
      "Number of tokens: 7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the number of tokens for each side\n",
    "max_len_right = math.floor(max_length / 2) + 1\n",
    "max_len_left = math.ceil(max_length / 2)\n",
    "\n",
    "# Actual length b/c dropping <bos> on each side (i.e., input_ids[1:])\n",
    "actual_right = max_len_right - 1\n",
    "actual_left = max_len_left - 1\n",
    "\n",
    "print(f'user specified -> max_length: {max_length}')\n",
    "print(f'max_len_right: {max_len_right}')\n",
    "print(f'max_len_left: {max_len_left}')\n",
    "print(f'actual_right: {actual_right}')\n",
    "print(f'actual_left: {actual_left}')\n",
    "print(f'Number of tokens: {actual_right + actual_left + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tk_left[\"length\"][0] - 1: 3\n",
      "tk_right[\"length\"][0] - 1: 3\n",
      "len(combine_input_ids): 6\n",
      "Number of Tokens: 7\n",
      "\n",
      "input_ids:\n",
      "[2, 1596, 603, 2793, 3287, 1281, 4381]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', 'This', '▁is', '▁text', '▁example', '▁use', '▁cases']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos>This is text example use cases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right side (same as approach 1 -> default)\n",
    "tk_right = tokenizer(text,\n",
    "                     max_length=max_len_right,\n",
    "                     truncation=True,\n",
    "                     return_length=True,\n",
    "                     padding=False)\n",
    "\n",
    "# Left side\n",
    "tokenizer.truncation_side = 'left'\n",
    "tk_left = tokenizer(text,\n",
    "                    max_length=max_len_left,\n",
    "                    truncation=True,\n",
    "                    return_length=True,\n",
    "                    padding=False)\n",
    "\n",
    "# Set truncation_side back to the default \"right\"\n",
    "tokenizer.truncation_side = 'right'\n",
    "\n",
    "# Combine right and left input_ids\n",
    "combine_input_ids = tk_right['input_ids'][1:] + tk_left['input_ids'][1:]\n",
    "combine_text = tokenizer.decode(combine_input_ids)\n",
    "\n",
    "# Tokenizer\n",
    "tk = tokenizer(combine_text,\n",
    "               max_length=max_length,\n",
    "               truncation=True,\n",
    "               return_length=True,\n",
    "               padding=False)\n",
    "print(f'tk_left[\"length\"][0] - 1: {tk_left[\"length\"][0] - 1}')\n",
    "print(f'tk_right[\"length\"][0] - 1: {tk_right[\"length\"][0] - 1}')\n",
    "print(f'len(combine_input_ids): {len(combine_input_ids)}')\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-3-chunking-overlap\">Approach 3: Chunking with Overlap</h1>\n",
    "\n",
    "In Hugging Face tokenizers, the return_overflowing_tokens and stride arguments are used together to handle long texts by splitting them into smaller chunks that the model can process.\n",
    "\n",
    "`return_overflowing_tokens`: When set to True, this argument instructs the tokenizer to return tokens that \"overflow\" beyond the specified max_length. This means if a text is too long to fit within the max_length, it will be split into multiple chunks, and each chunk will be returned as part of the output.\n",
    "\n",
    "`stride`: This argument defines the number of tokens to overlap between consecutive chunks. For example, if stride is set to 5 and max_length is 10, the tokenizer will create chunks that overlap by 5 tokens. This is useful for maintaining context across chunks, especially in tasks like question answering, where understanding context is critical.\n",
    "\n",
    "Together, these arguments allow for efficient processing of long texts by creating overlapping chunks, ensuring that important contextual information is preserved across the boundaries of the chunks.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks not accounting for stride: 2.2857142857142856\n"
     ]
    }
   ],
   "source": [
    "# Total number of tokens / max_length\n",
    "print(f'Number of chunks not accounting for stride: '\n",
    "      f'{total_tokens_text / max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 4\n",
      "1 of 4 (7): [2, 1596, 603, 2793, 674]\n",
      "2 of 4 (7): [2, 877, 614, 788, 129632]\n",
      "3 of 4 (7): [2, 675, 40768, 604, 3890]\n",
      "4 of 4 (4): [2, 3287, 1281, 4381]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer with stride\n",
    "tk = tokenizer(text,\n",
    "               max_length=max_length,\n",
    "               stride=stride,\n",
    "               truncation=True,\n",
    "               return_overflowing_tokens=True,\n",
    "               return_length=True,\n",
    "               padding=False,\n",
    "               )\n",
    "print(f'Number of Chunks: {len(tk[\"input_ids\"])}')\n",
    "for ii, input_ids in enumerate(tk['input_ids']):\n",
    "    print(f'{ii + 1} of {len(tk[\"input_ids\"])} ({tk[\"length\"][ii]}): {tk[\"input_ids\"][ii][:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns the `<bos>` token at the start of each chunk. To incorporate each chunk of text into the chat template a custom approach will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CHUNK OF DATA\n",
      "Number of Tokens: 7\n",
      "\n",
      "input_ids:\n",
      "[2, 1596, 603, 2793, 674, 877, 614]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', 'This', '▁is', '▁text', '▁that', '▁will', '▁be']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos>This is text that will be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First chunk of data\n",
    "print('FIRST CHUNK OF DATA')\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST CHUNK OF DATA\n",
      "Number of Tokens: 4\n",
      "\n",
      "input_ids:\n",
      "[2, 3287, 1281, 4381]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '▁example', '▁use', '▁cases']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos> example use cases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last chunk of data\n",
    "print('LAST CHUNK OF DATA')\n",
    "print(f'Number of Tokens: {tk[\"length\"][-1]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"takeaways\">Takeaways</h1>\n",
    "\n",
    "Please feel free to experiment with this notebook and tokenizers. Checkout:\n",
    "- The approaches in this notebook have been coded into the [/src/preprocess.py](./src/preprocess.py) module. \n",
    "- Try out the [dataset-and-collator.ipynb](./dataset-and-collator.ipynb) to see more about the next step in the text processing pipeline which is forming datasets and using collators.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
