{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"intro\">Text Tokenization - Three Data Splitting Approaches with a Chat Template</h1>\n",
    "\n",
    "This notebook provides a walkthrough of using [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating) and three text splitting approaches. The example in this notebook is identical to the [tokenizer-three-approaches.ipynb](./tokenizer-three-approaches.ipynb) **EXCEPT** this notebook **INCLUDES** a `chat template` which requires additional steps. The concepts in this notebook can be used to gain familiarity with tokenizers which are fundamental for NLP.\n",
    "\n",
    "## Table of Contents\n",
    "If viewing this notebook from GitHub please view it instead on [nbviewer.org](https://nbviewer.org/) so the hyperlinks will function. \n",
    "\n",
    "- [User Inputs](#user-inputs)\n",
    "- [Import Libraries and Modules](#import-libs)\n",
    "- [Tokenizer](#tokenizer)\n",
    "- [Chat Template](#chat-template)\n",
    "- [Approach 1: Right Side Truncation](#approach-1-right-side-truncation)\n",
    "- [Approach 2: Right and Left Side of Text](#approach-2-right-left-side-truncation)\n",
    "- [Approach 3: Chunking with Overlap](#approach-3-chunking-overlap)\n",
    "- [Takeaways](#takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"user-inputs\">User Inputs</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'gemma-2-9b-it'\n",
    "\n",
    "# Tokenizer max length and stride\n",
    "max_length = 8\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"import-libs\">Import Libraries and Modules</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Setup HF env. variables\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for this notebook\n",
    "text = (f'This is text that will be chunked '\n",
    "        f'with overlap for simple example use cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tokenizer\">Tokenizer</h1>\n",
    "\n",
    "The Gemma 2 model uses a [SentencePiece tokenizer](https://arxiv.org/html/2408.00118v1) with a vocabulary size of 256K.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token in Vocabulary: 256,000\n"
     ]
    }
   ],
   "source": [
    "# Path to the model and tokenizer model card saved on disk\n",
    "model_path = Path(os.getenv('LLM_MODELS')) / model_name\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Number of tokens in the tokenizer\n",
    "print(f'Number of Token in Vocabulary: {len(tokenizer.get_vocab()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in Hugging Face tokenizers are used to indicate the start and end of a sequence, separate sentences, or represent padding and unknown tokens. These tokens help the model understand the structure and boundaries of the input text during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens\n",
      "bos_token: <bos> -> 2\n",
      "eos_token: <eos> -> 1\n",
      "unk_token: <unk> -> 3\n",
      "sep_token: None -> None\n",
      "pad_token: <pad> -> 0\n",
      "cls_token: None -> None\n",
      "mask_token: None -> None\n",
      "additional_special_tokens: ['<start_of_turn>', '<end_of_turn>'] -> None\n",
      "\n",
      "Additional Special Tokens\n",
      "106: <start_of_turn>\n",
      "107: <end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "print('Special Tokens')\n",
    "for token_name in tokenizer.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "    token = getattr(tokenizer, token_name)\n",
    "    if token_name == 'additional_special_tokens':\n",
    "        token_id = None\n",
    "    else:\n",
    "        token_id = getattr(tokenizer, f'{token_name}_id')\n",
    "    print(f'{token_name}: {token} -> {token_id}')\n",
    "\n",
    "# Additional special tokens\n",
    "print(f'\\nAdditional Special Tokens')\n",
    "for token_id in tokenizer.additional_special_tokens_ids:\n",
    "    print(f'{token_id}: {tokenizer.decode(token_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def view_tokens(tok: Type[T], input_ids: list) -> None:\n",
    "    \"\"\"Decode and Print input_ids tokens for viewing \n",
    "\n",
    "    Args:\n",
    "        tok (Type[T]): Tokenizer\n",
    "        input_ids (list): List of input_id tokens\n",
    "    \"\"\"\n",
    "    print(f'input_ids:\\n{input_ids}\\n\\n')\n",
    "    print(f'tokenizer.convert_ids_to_tokens(input_ids):\\n'\n",
    "          f'{tok.convert_ids_to_tokens(input_ids)}\\n\\n')\n",
    "    \n",
    "    print(f'tokenizer.decode(input_ids):\\n'\n",
    "          f'{tok.decode(input_ids)}\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"chat-template\">Chat Template</h1>\n",
    "\n",
    "The chat template in Hugging Face models is used to structure and format the input text for conversational AI models. It defines how the user and assistant inputs should be formatted, including prefixes, separators, and special tokens, ensuring that the model can properly differentiate between user prompts and assistant responses. This template helps maintain the context and flow of the conversation during training and inference.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tk): 3\n",
      "\n",
      "tk.keys(): dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "\n",
      "Number of Tokens: 6\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 107, 108]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', '<end_of_turn>', '\\n']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat template\n",
    "empty_text = ''\n",
    "message = [{'role': 'user', 'content': empty_text}]\n",
    "tk = tokenizer.apply_chat_template(conversation=message,\n",
    "                                   tokenize=True,\n",
    "                                   add_generation_prompt=False,\n",
    "                                   return_dict=True,\n",
    "                                   tokenizer_kwargs={'return_length': True}\n",
    "                                   )\n",
    "\n",
    "# Number of tokens in the chat template\n",
    "num_token_chat_template = tk['length'][0]\n",
    "\n",
    "print(f'len(tk): {len(tk)}\\n')\n",
    "print(f'tk.keys(): {tk.keys()}\\n')\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tk): 3\n",
      "\n",
      "tk.keys(): dict_keys(['input_ids', 'attention_mask', 'length'])\n",
      "\n",
      "Number of Tokens: 21\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 1596, 603, 2793, 674, 877, 614, 788, 129632, 675, 40768, 604, 3890, 3287, 1281, 4381, 107, 108]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'This', '▁is', '▁text', '▁that', '▁will', '▁be', '▁ch', 'unked', '▁with', '▁overlap', '▁for', '▁simple', '▁example', '▁use', '▁cases', '<end_of_turn>', '\\n']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "This is text that will be chunked with overlap for simple example use cases<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer the entire text\n",
    "message = [{'role': 'user', 'content': text}]\n",
    "tk = tokenizer.apply_chat_template(conversation=message,\n",
    "                                   tokenize=True,\n",
    "                                   add_generation_prompt=False,\n",
    "                                   return_dict=True,\n",
    "                                   tokenizer_kwargs={'return_length': True}\n",
    "                                   )\n",
    "\n",
    "# Number of tokens in the text\n",
    "total_tokens_text = tk[\"length\"][0]\n",
    "\n",
    "print(f'len(tk): {len(tk)}\\n')\n",
    "print(f'tk.keys(): {tk.keys()}\\n')\n",
    "print(f'Number of Tokens: {total_tokens_text}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-1-right-side-truncation\">Approach 1: Right Side Truncation</h1>\n",
    "\n",
    "\n",
    "This apporach takes the first portion of text (from 0 to max_length) and is the most common approach found in tokenizing text.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 8\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 1596, 603, 2793, 674]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'This', '▁is', '▁text', '▁that']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "This is text that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep the right side of the text\n",
    "message = [{'role': 'user', 'content': text}]\n",
    "tk = tokenizer.apply_chat_template(conversation=message,\n",
    "                                   tokenize=True,\n",
    "                                   add_generation_prompt=False,\n",
    "                                   return_dict=True,\n",
    "                                   max_length=max_length,\n",
    "                                   truncation=True,\n",
    "                                   tokenizer_kwargs={'return_length': True}\n",
    "                                   )\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-2-right-left-side-truncation\">Approach 2: Right and Left Side Truncation</h1>\n",
    "\n",
    "In this approach the right and left side of the text will be taken and combined together. This will require two separate tokenizations of the text, recombining it, and then tokenizing in the chat template.\n",
    "\n",
    "Some math needs to be performed to account for the max_length to take from each side. When doing this account for:\n",
    "- Number of tokens to take for each side (i.e., max_length/2),\n",
    "- Number of tokens in the chat template (-1 because it also starts with `<bos>` token),\n",
    "- The `<bos>` token will be dropped at the start of each side of text.\n",
    "\n",
    "In the below cell notice the use of `math.floor` and `math.ceil` which are used if an odd `max_length` is set. This will ensure that the returned number of tokens is exactly `max_length`. \n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user specified -> max_length: 8\n",
      "max_len_right: 2\n",
      "max_len_left: 3\n",
      "actual_right: 1\n",
      "actual_left: 2\n",
      "Number of tokens: 8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Offset accounts for input_ids[1:] for each side\n",
    "offset = 1\n",
    "\n",
    "# Calculate the number of tokens for each side\n",
    "max_len_right = math.floor(((max_length - (num_token_chat_template - 1)) / 2)) + offset\n",
    "max_len_left = math.ceil(((max_length - (num_token_chat_template - 1)) / 2)) + offset\n",
    "\n",
    "# Actual length b/c dropping <bos> on each side (i.e., input_ids[1:])\n",
    "actual_right = max_len_right - 1\n",
    "actual_left = max_len_left - 1\n",
    "\n",
    "print(f'user specified -> max_length: {max_length}')\n",
    "print(f'max_len_right: {max_len_right}')\n",
    "print(f'max_len_left: {max_len_left}')\n",
    "print(f'actual_right: {actual_right}')\n",
    "print(f'actual_left: {actual_left}')\n",
    "print(f'Number of tokens: {actual_right + actual_left + (num_token_chat_template - 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tk_left[\"length\"][0] - 1: 2\n",
      "tk_right[\"length\"][0] - 1: 1\n",
      "len(combine_input_ids): 3\n",
      "Number of Tokens: 9\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 1596, 1281, 4381, 107, 108]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'This', '▁use', '▁cases', '<end_of_turn>', '\\n']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "This use cases<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right side (same as approach 1 -> default)\n",
    "tk_right = tokenizer(text,\n",
    "                     max_length=max_len_right,\n",
    "                     truncation=True,\n",
    "                     return_length=True,\n",
    "                     padding=False)\n",
    "\n",
    "# Left side\n",
    "tokenizer.truncation_side = 'left'\n",
    "tk_left = tokenizer(text,\n",
    "                    max_length=max_len_left,\n",
    "                    truncation=True,\n",
    "                    return_length=True,\n",
    "                    padding=False)\n",
    "\n",
    "# Set truncation_side back to the default \"right\"\n",
    "tokenizer.truncation_side = 'right'\n",
    "\n",
    "# Combine right and left input_ids\n",
    "combine_input_ids = tk_right['input_ids'][1:] + tk_left['input_ids'][1:]\n",
    "combine_text = tokenizer.decode(combine_input_ids)\n",
    "\n",
    "# Tokenizer\n",
    "message = [{'role': 'user', 'content': combine_text}]\n",
    "tk = tokenizer.apply_chat_template(conversation=message,\n",
    "                                   tokenize=True,\n",
    "                                   add_generation_prompt=False,\n",
    "                                   return_dict=True,\n",
    "                                   tokenizer_kwargs={'return_length': True}\n",
    "                                   )\n",
    "print(f'tk_left[\"length\"][0] - 1: {tk_left[\"length\"][0] - 1}')\n",
    "print(f'tk_right[\"length\"][0] - 1: {tk_right[\"length\"][0] - 1}')\n",
    "print(f'len(combine_input_ids): {len(combine_input_ids)}')\n",
    "print(f'Number of Tokens: {tk[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=tk['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"approach-3-chunking-overlap\">Approach 3: Chunking with Overlap</h1>\n",
    "\n",
    "In Hugging Face tokenizers, the return_overflowing_tokens and stride arguments are used together to handle long texts by splitting them into smaller chunks that the model can process.\n",
    "\n",
    "`return_overflowing_tokens`: When set to True, this argument instructs the tokenizer to return tokens that \"overflow\" beyond the specified max_length. This means if a text is too long to fit within the max_length, it will be split into multiple chunks, and each chunk will be returned as part of the output.\n",
    "\n",
    "`stride`: This argument defines the number of tokens to overlap between consecutive chunks. For example, if stride is set to 5 and max_length is 10, the tokenizer will create chunks that overlap by 5 tokens. This is useful for maintaining context across chunks, especially in tasks like question answering, where understanding context is critical.\n",
    "\n",
    "Together, these arguments allow for efficient processing of long texts by creating overlapping chunks, ensuring that important contextual information is preserved across the boundaries of the chunks.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks not accounting for stride: 2.625\n"
     ]
    }
   ],
   "source": [
    "# Total number of tokens / max_length\n",
    "print(f'Number of chunks not accounting for stride: '\n",
    "      f'{total_tokens_text / max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 3\n",
      "1 of 3: [2, 106, 1645, 108, 1596]\n",
      "2 of 3: [674, 877, 614, 788, 129632]\n",
      "3 of 3: [604, 3890, 3287, 1281, 4381]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "message = [{'role': 'user', 'content': text}]\n",
    "tk = tokenizer.apply_chat_template(conversation=message,\n",
    "                                   tokenize=True,\n",
    "                                   add_generation_prompt=False,\n",
    "                                   return_dict=True,\n",
    "                                   max_length=max_length,\n",
    "                                   truncation=True,\n",
    "                                   padding=False,\n",
    "                                   tokenizer_kwargs={'return_length': True,\n",
    "                                                     'stride': stride,\n",
    "                                                     'return_overflowing_tokens': True}\n",
    "                                   )\n",
    "print(f'Number of Chunks: {len(tk[\"input_ids\"])}')\n",
    "for ii, input_ids in enumerate(tk['input_ids']):\n",
    "    print(f'{ii + 1} of {len(tk[\"input_ids\"])}: {tk[\"input_ids\"][ii][:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell only the first chunk has a `<bos>` token which means that when using stride in the `apply_chat_template` it will chunk the entire chat template. Let's observe how tokenizer with stride chunks the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 3\n",
      "1 of 3 (8): [2, 1596, 603, 2793, 674]\n",
      "2 of 3 (8): [2, 788, 129632, 675, 40768]\n",
      "3 of 3 (4): [2, 3287, 1281, 4381]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer with stride\n",
    "tk = tokenizer(text,\n",
    "               max_length=max_length,\n",
    "               stride=stride,\n",
    "               truncation=True,\n",
    "               return_overflowing_tokens=True,\n",
    "               return_length=True,\n",
    "               padding=False,\n",
    "               )\n",
    "print(f'Number of Chunks: {len(tk[\"input_ids\"])}')\n",
    "for ii, input_ids in enumerate(tk['input_ids']):\n",
    "    print(f'{ii + 1} of {len(tk[\"input_ids\"])} ({tk[\"length\"][ii]}): {tk[\"input_ids\"][ii][:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns the `<bos>` token at the start of each chunk. To incorporate each chunk of text into the chat template a custom approach will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer with stride\n",
    "tk = tokenizer(text,\n",
    "               max_length=max_length - (num_token_chat_template - 1),\n",
    "               stride=stride,\n",
    "               truncation=True,\n",
    "               return_overflowing_tokens=True,\n",
    "               return_length=True,\n",
    "               padding=False,\n",
    "               )\n",
    "\n",
    "results = {'input_ids': [],\n",
    "           'attention_mask': [],\n",
    "           'length': []}\n",
    "for ii, input_id in enumerate(tk['input_ids']):\n",
    "    message = [{'role': 'user',\n",
    "                'content': tokenizer.decode(input_id[1:])}]\n",
    "    tk_chat = tokenizer.apply_chat_template(\n",
    "        conversation=message,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        tokenizer_kwargs={'return_length': True})\n",
    "    for field in ['input_ids', 'attention_mask', 'length']:\n",
    "        results[field] += [tk_chat[field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST CHUNK OF DATA\n",
      "Number of Tokens: [8]\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 1596, 603, 107, 108]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'This', '▁is', '<end_of_turn>', '\\n']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "This is<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First chunk of data\n",
    "print('FIRST CHUNK OF DATA')\n",
    "print(f'Number of Tokens: {results[\"length\"][0]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=results['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST CHUNK OF DATA\n",
      "Number of Tokens: [8]\n",
      "\n",
      "input_ids:\n",
      "[2, 106, 1645, 108, 1589, 4381, 107, 108]\n",
      "\n",
      "\n",
      "tokenizer.convert_ids_to_tokens(input_ids):\n",
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'use', '▁cases', '<end_of_turn>', '\\n']\n",
      "\n",
      "\n",
      "tokenizer.decode(input_ids):\n",
      "<bos><start_of_turn>user\n",
      "use cases<end_of_turn>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last chunk of data\n",
    "print('LAST CHUNK OF DATA')\n",
    "print(f'Number of Tokens: {results[\"length\"][-1]}\\n')\n",
    "view_tokens(tok=tokenizer, input_ids=results['input_ids'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"takeaways\">Takeaways</h1>\n",
    "\n",
    "Please feel free to experiment with this notebook and tokenizers. Checkout:\n",
    "- The approaches in this notebook have been coded into the [/src/preprocess.py](./src/preprocess.py) module. \n",
    "- Try out the [dataset-and-collator.ipynb](./dataset-and-collator.ipynb) to see more about the next step in the text processing pipeline which is forming datasets and using collators.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
