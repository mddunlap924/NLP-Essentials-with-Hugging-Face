{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"intro\">Text Preprocessing: Data Collator</h1>\n",
    "\n",
    "\n",
    "This notebook demonstrates the use of a Hugging Face tokenizer and how to effectively utilize a data collator to prepare inputs for a model. The key focus is on the role of the collator in dynamically batching and padding tokenized sequences, ensuring that the data is consistently formatted for model training and inference.\n",
    "\n",
    "## Table of Contents\n",
    "If viewing this notebook from GitHub please view it instead on [nbviewer.org](https://nbviewer.org/) so the hyperlinks will function. \n",
    "\n",
    "- [User Inputs](#user-inputs)\n",
    "- [Import Libraries and Modules](#import-libs)\n",
    "- [Tokenizer](#tokenizer)\n",
    "- [Collator and Dataloader](#collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"user-inputs\">User Inputs</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'gemma-2-9b-it'\n",
    "\n",
    "# Tokenizer max length and stride\n",
    "max_length = 7\n",
    "stride = 2\n",
    "\n",
    "# Example text to use for demonstration\n",
    "texts = [\n",
    "\"The quick brown fox jumps over the lazy dog.\",\n",
    "\"Artificial intelligence is transforming industries\",\n",
    "\"Python is a versatile programming language loved by developers\",\n",
    "\"In 2024, self-driving cars may become more common on the streets.\",\n",
    "\"Does the age and gender get tokenized for a 20F and 20M or is it unknown\"\n",
    "]\n",
    "\n",
    "# Polars set column widths for display\n",
    "fmt_str_lengths = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"import-libs\">Import Libraries and Modules</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Setup HF env. variables\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'True'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "# Custom modules\n",
    "# from src.preprocess import custom_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>text</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;The quick brown fox jumps over the lazy dog.&quot;</td></tr><tr><td>1</td><td>&quot;Artificial intelligence is transforming industries&quot;</td></tr><tr><td>2</td><td>&quot;Python is a versatile programming language loved by developers&quot;</td></tr><tr><td>3</td><td>&quot;In 2024, self-driving cars may become more common on the streets.&quot;</td></tr><tr><td>4</td><td>&quot;Does the age and gender get tokenized for a 20F and 20M or is it unknown&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────┬──────────────────────────────────────────────────────────────────────────┐\n",
       "│ id  ┆ text                                                                     │\n",
       "│ --- ┆ ---                                                                      │\n",
       "│ i64 ┆ str                                                                      │\n",
       "╞═════╪══════════════════════════════════════════════════════════════════════════╡\n",
       "│ 0   ┆ The quick brown fox jumps over the lazy dog.                             │\n",
       "│ 1   ┆ Artificial intelligence is transforming industries                       │\n",
       "│ 2   ┆ Python is a versatile programming language loved by developers           │\n",
       "│ 3   ┆ In 2024, self-driving cars may become more common on the streets.        │\n",
       "│ 4   ┆ Does the age and gender get tokenized for a 20F and 20M or is it unknown │\n",
       "└─────┴──────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Place the text into a dataframe and given each sample an ID\n",
    "df = pl.DataFrame({'id': [i for i in range(len(texts))],\n",
    "                   'text': texts})\n",
    "with pl.Config(fmt_str_lengths=fmt_str_lengths):\n",
    "    display(df.head())\n",
    "\n",
    "del texts\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tokenizer\">Tokenizer</h1>\n",
    "\n",
    "The Gemma 2 model uses a [SentencePiece tokenizer](https://arxiv.org/html/2408.00118v1) with a vocabulary size of 256K.\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token in Vocabulary: 256,000\n"
     ]
    }
   ],
   "source": [
    "# Path to the model and tokenizer model card saved on disk\n",
    "model_path = Path(os.getenv('LLM_MODELS')) / model_name\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Number of tokens in the tokenizer\n",
    "print(f'Number of Token in Vocabulary: {len(tokenizer.get_vocab()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>text</th><th>num_tokens</th></tr><tr><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;The quick brown fox jumps over the lazy dog.&quot;</td><td>11</td></tr><tr><td>1</td><td>&quot;Artificial intelligence is transforming industries&quot;</td><td>6</td></tr><tr><td>2</td><td>&quot;Python is a versatile programming language loved by developers&quot;</td><td>10</td></tr><tr><td>3</td><td>&quot;In 2024, self-driving cars may become more common on the streets.&quot;</td><td>20</td></tr><tr><td>4</td><td>&quot;Does the age and gender get tokenized for a 20F and 20M or is it unknown&quot;</td><td>24</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────┬──────────────────────────────────────────────────────────────────────────┬────────────┐\n",
       "│ id  ┆ text                                                                     ┆ num_tokens │\n",
       "│ --- ┆ ---                                                                      ┆ ---        │\n",
       "│ i64 ┆ str                                                                      ┆ i64        │\n",
       "╞═════╪══════════════════════════════════════════════════════════════════════════╪════════════╡\n",
       "│ 0   ┆ The quick brown fox jumps over the lazy dog.                             ┆ 11         │\n",
       "│ 1   ┆ Artificial intelligence is transforming industries                       ┆ 6          │\n",
       "│ 2   ┆ Python is a versatile programming language loved by developers           ┆ 10         │\n",
       "│ 3   ┆ In 2024, self-driving cars may become more common on the streets.        ┆ 20         │\n",
       "│ 4   ┆ Does the age and gender get tokenized for a 20F and 20M or is it unknown ┆ 24         │\n",
       "└─────┴──────────────────────────────────────────────────────────────────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Type, TypeVar, List, Dict, Tuple\n",
    "T = TypeVar('T')\n",
    "\n",
    "\n",
    "def count_tokens(tok: Type[T], text: str) -> int:\n",
    "    tk = tok(text, truncation=False, return_length=True)\n",
    "    return tk['length'][0]\n",
    "\n",
    "\n",
    "# Number of tokens in each text\n",
    "df = df.with_columns(\n",
    "    pl.col('text')\n",
    "    .map_elements(lambda x: count_tokens(tok=tokenizer, text=x),\n",
    "                  return_dtype=pl.Int64)\n",
    "    .alias('num_tokens')\n",
    ")\n",
    "with pl.Config(fmt_str_lengths=fmt_str_lengths):\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text', 'num_tokens'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataframe into a dataset\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_polars(df)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [src.preprocess.basic.py](../src/preprocess/basic.py) there are two examples of tokenizing text for a `Dataset`:\n",
    "1) **Function**: see `left_or_right`.\n",
    "2) **Class**: see `CustomTokenizer`.\n",
    "\n",
    "If you would prefer to use Option 1 and tokenize the text using the `left_or_right` function it would be called as shown.\n",
    "\n",
    "```python\n",
    "# Tokenization as a function\n",
    "ds = ds.map(function=left_or_right,\n",
    "            fn_kwargs={'tokenizer': tokenizer,\n",
    "                       'cfg': CFG},\n",
    "            num_proc=num_proc,\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1ae2b293f3469fbe082c37cfcd036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Number of Samples: 5\n",
      "Column Names: ['id', 'text', 'num_tokens', 'input_ids', 'attention_mask', 'length']\n",
      "Max Length in Dataset: [24]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Type, TypeVar, List, Dict, Tuple, Union\n",
    "\n",
    "import importlib\n",
    "from src.preprocess import basic\n",
    "importlib.reload(basic)\n",
    "from src.preprocess.basic import CustomTokenizer\n",
    "from src.preprocess.basic import left_or_right\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Class for storing user configurable parameters\"\"\"\n",
    "    max_length: Union[int, None]=4\n",
    "    truncation: bool=False\n",
    "    padding: bool=False\n",
    "    return_length: bool=True\n",
    "    truncation_side: str='right'\n",
    "    num_proc: int=2\n",
    "    cols_select: list=field(default_factory=lambda: ['input_ids',\n",
    "                                                     'attention_mask',\n",
    "                                                     'length'])\n",
    "    batch_size: int=2\n",
    "\n",
    "# User configurations\n",
    "CFG = Config(max_length=None)\n",
    "\n",
    "# Tokenization custom class\n",
    "tok = CustomTokenizer(cfg=CFG, tokenizer=tokenizer)\n",
    "ds = ds.map(function=tok.left_or_right,\n",
    "            num_proc=CFG.num_proc,\n",
    "            )\n",
    "\n",
    "print(f'Dataset Number of Samples: {len(ds)}')\n",
    "print(f'Column Names: {ds.column_names}')\n",
    "print(f'Max Length in Dataset: {max(ds[\"length\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"collator\">Collator and Dataloader</h1>\n",
    "\n",
    "##### [Return To Top](#intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer,\n",
    "                                   return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples / Batch Size: 2.5\n",
      "Number of batches of data: 3\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Loader\n",
    "dl = DataLoader(dataset=ds.select_columns(CFG.cols_select),\n",
    "                batch_size=CFG.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collator,\n",
    "                num_workers=CFG.num_proc)\n",
    "print(f'Number of Samples / Batch Size: {len(ds) / CFG.batch_size}')\n",
    "print(f'Number of batches of data: {len(dl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Num. of Samples: 2\n",
      "Sample 1 of 2 in Batch\n",
      "\tLength of input_ids AFTER collator: 11\n",
      "\tLength of input_ids BEFORE collator: 11\n",
      "Sample 2 of 2 in Batch\n",
      "\tLength of input_ids AFTER collator: 11\n",
      "\tLength of input_ids BEFORE collator: 6\n"
     ]
    }
   ],
   "source": [
    "# Sample a batch of data from the dataloader\n",
    "batch = next(iter(dl))\n",
    "\n",
    "# Check the batch size\n",
    "print(f'Batch Num. of Samples: {len(batch[\"input_ids\"])}')\n",
    "\n",
    "# Check the vector length of the batch\n",
    "for ii, (input_ids, lengths) in enumerate(zip(batch['input_ids'], batch['length'])):\n",
    "    print(f'Sample {ii + 1} of {len(batch[\"input_ids\"])} in Batch')\n",
    "    print(f'\\tLength of input_ids AFTER collator: {len(input_ids):,}')\n",
    "    print(f'\\tLength of input_ids BEFORE collator: {lengths[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above demonstrates how the collator is padding the second sample from a token length of 6 to the token length of 11 for the first sample.\n",
    "\n",
    "At this point the data can be consumed as batches into a model for either inference or training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
